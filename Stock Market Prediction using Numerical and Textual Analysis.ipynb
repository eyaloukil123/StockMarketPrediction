{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eya Loukil\n",
    "\n",
    "<br>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"stock_market.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"tsf.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "## TSF Task 7 : Stock Market Prediction using Numerical and Textual Analysis\n",
    "### (Level - Advanced)\n",
    "\n",
    "\n",
    "● Objective: Creating a hybrid model for stock price/performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines <br>\n",
    "● Stock to analyze and predict - S&P BSE SENSEX <br>\n",
    "● Download the historical stock prices from finance.yahoo.com <br>\n",
    "● Download the textual (news) data from https://bit.ly/36fFPI6 <br>\n",
    "● Using Python for separate analysis and then combine the findings to create a hybrid model <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip3 install yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Dense, Activation\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the historic data of SENSEX from 2001 to 2020\n",
    "\n",
    "df_price = yf.download('^BSESN', start='2001-01-02', end='2021-01-01')\n",
    "df_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['Open'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['High'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['Low'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['Close'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['Adj Close'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['Volume'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_price.columns)\n",
    "df_price.reset_index(inplace=True)\n",
    "df_price.rename(columns={'Date': 'date','Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Adj Close': 'adjclose', 'Volume': 'volume'}, inplace = True)\n",
    "df_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the textual news data\n",
    "\n",
    "df_news=pd.read_csv(\"C:/Stock-Market-Prediction-using-Numerical-and-Textual-Analysis-main/Stock-Market-Prediction-using-Numerical-and-Textual-Analysis-main/stock_data.csv\")\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_price), len(df_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price.isnull().count(), df_news.isnull().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis on SENSEX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing the SENSEX data\n",
    "\n",
    "df_price =df_price.drop_duplicates()\n",
    "df_price['date'] = pd.to_datetime(df_price['date'].dt.normalize())\n",
    "df_price = df_price.filter(['date', 'close', 'open', 'high', 'low', 'volume'])\n",
    "df_price.set_index('date', inplace= True)\n",
    "df_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis on Headlines data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news=df_news.drop_duplicates()\n",
    "df_news['publish_date'] = pd.to_datetime(df_news['publish_date'],format= '%Y%m%d').dt.normalize()\n",
    "df_news=df_news.filter(['publish_date','headline_text'])\n",
    "df_news=df_news.groupby(['publish_date'])['headline_text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "df_news.set_index('publish_date', inplace=True)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the required data in a single dataframe\n",
    "\n",
    "df_stock=pd.concat([df_price,df_news],axis=1)\n",
    "df_stock.dropna(axis=0, inplace=True)\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using nltk - vader library to analyse the sentiments based on the headline data\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_stock['Compound'] = [sia.polarity_scores(v)['compound'] for v in df_stock['headline_text']]\n",
    "df_stock['Negative'] = [sia.polarity_scores(v)['neg'] for v in df_stock['headline_text']]\n",
    "df_stock['Neutral'] = [sia.polarity_scores(v)['neu'] for v in df_stock['headline_text']]\n",
    "df_stock['Positive'] = [sia.polarity_scores(v)['pos'] for v in df_stock['headline_text']]\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing headline data\n",
    "\n",
    "df_stock.drop((['headline_text']), axis=1, inplace=True)\n",
    "df_stock = df_stock[['close', 'Compound', 'Negative', 'Neutral', 'Positive', 'open', 'high', 'low', 'volume']]\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the compiled dataframe in a excel sheet that can be used later\n",
    "\n",
    "df_stock.to_csv('stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the stock_data \n",
    "\n",
    "stock_data=pd.read_csv('stock_data.csv')\n",
    "stock_data.rename(columns={'Unnamed: 0':'Date'}, inplace = True)\n",
    "stock_data.set_index('Date', inplace=True)\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualing the close price over the period of analysis\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "stock_data['close'].plot(label='SENSEX')\n",
    "plt.title(\"SENSEX Close Price\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualing the 50-Day moving average for measuring stock performance over the period\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "stock_data['close'].plot(label='SENSEX')\n",
    "stock_data.rolling(window=50).mean()['close'].plot(label='50-DMA', color='r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Preparation For Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the number of datapoints for training and testing\n",
    "\n",
    "percentage_of_data = 1.0\n",
    "data_to_use = int(percentage_of_data*(len(stock_data)-1))\n",
    "\n",
    "train_end = int(data_to_use*0.8)\n",
    "total_data = len(stock_data)\n",
    "start = total_data - data_to_use\n",
    "\n",
    "print(\"Number of records in Training Data:\", train_end)\n",
    "print(\"Number of records in Test Data:\", total_data - train_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocating the datapoints for each column\n",
    "\n",
    "steps_to_predict = 1\n",
    "\n",
    "close_price = stock_data.iloc[start:total_data,0]        \n",
    "compound = stock_data.iloc[start:total_data,1]           \n",
    "negative = stock_data.iloc[start:total_data,2]           \n",
    "neutral = stock_data.iloc[start:total_data,3]            \n",
    "positive = stock_data.iloc[start:total_data,4]           \n",
    "open_price = stock_data.iloc[start:total_data,5]         \n",
    "high = stock_data.iloc[start:total_data,6]               \n",
    "low = stock_data.iloc[start:total_data,7]                \n",
    "volume = stock_data.iloc[start:total_data,8]             \n",
    "\n",
    "print(\"Close Price:\")\n",
    "close_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting next day close\n",
    "close_price_shifted = close_price.shift(-1) \n",
    "\n",
    "# shifting next day compound\n",
    "compound_shifted = compound.shift(-1) \n",
    "\n",
    "# concatenating the captured training data into a dataframe\n",
    "data = pd.concat([close_price, close_price_shifted, compound, compound_shifted, volume, open_price, high, low], axis=1)\n",
    "\n",
    "# setting column names of the revised stock data\n",
    "data.columns = ['close_price', 'close_price_shifted', 'compound', 'compound_shifted','volume', 'open_price', 'high', 'low']\n",
    "\n",
    "data = data.dropna()    \n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the target variable as the shifted close_price\n",
    "\n",
    "y = data['close_price_shifted']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the future dataset for training the model\n",
    "\n",
    "cols = ['close_price', 'compound', 'compound_shifted', 'volume', 'open_price', 'high', 'low']\n",
    "x = data[cols]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the feature dataset\n",
    "scaler_x = preprocessing.MinMaxScaler (feature_range=(-1, 1))\n",
    "x = np.array(x).reshape((len(x) ,len(cols)))\n",
    "x = scaler_x.fit_transform(x)\n",
    "\n",
    "# scaling the target variable\n",
    "scaler_y = preprocessing.MinMaxScaler (feature_range=(-1, 1))\n",
    "y = np.array (y).reshape ((len( y), 1))\n",
    "y = scaler_y.fit_transform (y)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In timeseries data, an observation for a particular date is always dependent on the previous date records and the data like stock prices which is dependent on date, the dataset is divided into train and test dataset as below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and test dataset\n",
    "\n",
    "X_train = x[0 : train_end,]\n",
    "X_test = x[train_end+1 : len(x),]    \n",
    "y_train = y[0 : train_end] \n",
    "y_test = y[train_end+1 : len(y)]  \n",
    "\n",
    "# printing the shape of the training and the test datasets\n",
    "\n",
    "print('Number of rows and columns in the Training set X:', X_train.shape, 'and y:', y_train.shape)\n",
    "print('Number of rows and columns in the Test set X:', X_test.shape, 'and y:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the feature dataset into 3D for feeding into the LSTM model\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape + (1,)) \n",
    "X_test = X_test.reshape(X_test.shape + (1,))\n",
    "\n",
    "# printing the re-shaped feature dataset\n",
    "print('Shape of Training set X:', X_train.shape)\n",
    "print('Shape of Test set X:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the seed to achieve consistent and less random predictions at each execution\n",
    "np.random.seed(2021)\n",
    "\n",
    "# setting the model architecture\n",
    "model=Sequential()\n",
    "model.add(LSTM(100,return_sequences=True,activation='tanh',input_shape=(len(cols),1)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(100,return_sequences=True,activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(100,activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# printing the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='mse' , optimizer='adam')\n",
    "\n",
    "# fitting the model using the training dataset\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction of stock data using the test dataset\n",
    "\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "# unscaling the predictions\n",
    "predictions = scaler_y.inverse_transform(np.array(predictions).reshape((len(predictions), 1)))\n",
    "\n",
    "# printing the predictions\n",
    "print('Predictions:')\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the training mean-squared-error\n",
    "train_loss = model.evaluate(X_train, y_train, batch_size = 1)\n",
    "\n",
    "# calculating the test mean-squared-error\n",
    "test_loss = model.evaluate(X_test, y_test, batch_size = 1)\n",
    "\n",
    "# printing the training and the test mean-squared-errors\n",
    "print('Train Loss =', round(train_loss,4))\n",
    "print('Test Loss =', round(test_loss,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating root mean squared error using sklearn.metrics package\n",
    "\n",
    "rmse = metrics.mean_squared_error(y_test, predictions)\n",
    "print('Root Mean Square Error (sklearn.metrics) =', round(np.sqrt(rmse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscaling the test datasets\n",
    "X_test = scaler_x.inverse_transform(np.array(X_test).reshape((len(X_test), len(cols))))\n",
    "\n",
    "# unscaling the test y dataset, y_test\n",
    "y_train = scaler_y.inverse_transform(np.array(y_train).reshape((len(y_train), 1)))\n",
    "y_test = scaler_y.inverse_transform(np.array(y_test).reshape((len(y_test), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the prediction and original dataset on the same plot\n",
    "    \n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(predictions, label=\"Predicted Close Price\", color='r')\n",
    "plt.plot([row[0] for row in y_test], label=\"Testing Close Price\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "plt.title('SENSEX and Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
